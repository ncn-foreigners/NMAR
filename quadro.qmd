---
title: "NMAR"
format: 
  revealjs:
    theme: ["./quadro/q-theme.scss"]
    code-overflow: wrap
    highlight-style: a11y
    height: 1080
    width: 1920    
    scrollable: true
    html-math-method: mathjax
editor: visual
---

## NMAR: An R Package for Estimation under Nonignorable Nonresponse
::: {.hidden}
$$\everymath{\displaystyle}$$
:::

### Tackling Nonignorable Missingness in Official Surveys with R

<hr>

#### National Science Centre grant Poland (OPUS 20 grant no. 2020/39/B/HS4/00941)

<br>

#### dr Maciej BerÄ™sewicz (Poznan University of Business and Economics),<br> Igor KoÅ‚odziej (WUT - Faculty of Mathematics),<br> Mateusz Iwaniuk (WUT - Faculty of Mathematics)

<br>

> NMAR - Not Missing At Random

::: {style="text-align: right;"}
<img src="./logo.PNG" alt="Logo" style="height: 550px;position:fixed;right:50px;bottom:100px"/>
:::

## Motivation: The Challenge of Nonignorable Nonresponse (NMAR)

1.  **Missing Data Structure**: We are interesting in estimating the mean of $Y$ which is subject to missingness. Each observation $y_i$ has its corresponding respone indicator $\delta_i$:

> $$
> \delta_i = \begin{cases} 
> 1 & \text{if } y_i \text{ is observed} \\
> 0 & \text{if } y_i \text{ is missing}
> \end{cases}
> $$

2.  **Outcome Mechanism**: Having a set of auxiliary covariates $X_1$, we model

$$\mathbb{P}(Y \mid X_1) \text{, or nonparametric } F(Y, X_1)$$
    
3.  **Response Mechanism**: The missingness of $Y$ depends on $Y$ itself, and perhaps on other covariates $X_2$

$$P(\delta=1\mid Y, X_2) \neq P(\delta=1\mid X_2)$$

**Note**: The sets of auxiliary and response model covariates may or may not overlap depending the method.

## Practical Example: Salary Survey

**Scenario**: Richer people tend not to answer questions about salary

> $Y$: Salary (with missing values) <br> $\delta$: Response indicator (1 = answered, 0 = refused)

<br>

**Outcome Mechanism** :

> $Y \sim \text{experience} + \text{education}$

<br>

**Response Mechanism**:

Assuming that apart from salary, it is also affected by age

> $\delta \sim Y + \text{age}$

## But does it work?

::: {style="text-align: center;"}
<img src="./quadro/boxplots.png" alt="Simulation Results" style="width: 70%;"/>
:::

> NMAR estimators are closer to true population mean compared to MAR estimators, naive sample mean is badly biased.

## Overview of the NMAR R Package

### A Production-Grade Implementation of NMAR Estimators

**Package Architecture: Plugin-Based Design**

```{r}
#| eval: false
#| echo: true
# Simple, unified interface
result <- nmar(
  formula = Y_miss ~ X1 + X2, # Outcome and auxiliaries
  data = my_data, # data.frame or survey.design
  engine = el_engine(...), # Choose your method
  response_predictors = c("Z") # Response mechanism predictors
)
```

**Three estimation engines:** - `el_engine()` â€” Empirical Likelihood (Qin, Leung, and Shao 2002) - `exptilt_engine()` â€” Exponential Tilting (Kim and Riddles 2012) - `exptilt_nonparam_engine()` â€” Nonparametric Exponential Tilting (Riddles et al. 2016)

------------------------------------------------------------------------

## Package Design Philosophy {.smaller}

### Separation of Concerns and Extensibility

**Core Design Principles:**

1.  **Single entry point** â€” `nmar()` function for all methods.
2.  **Engine-agnostic interface** â€” Swap methods without changing user code.
3.  **Dual data support** â€” Handles both `data.frame` (IID) and `survey.design` (complex surveys) with correct, design-based inference.
4.  **Modular architecture** â€” Clean separation between:
    -   Input validation (`validation/`)
    -   Data preparation (`shared/input_pipeline.R`)
    -   Engine implementations (`engines/*/`)
    -   Variance estimation (`shared/bootstrap.R`, `engines/*/impl/variance.R`)
    -   Result objects and S3 methods (`S3/`)

**Why this matters:** - **Reproducibility:** Researchers can compare methods on identical data. - **Extensibility:** New engines can be added without touching existing code. - **Compatiblity:** Survey statisticians get design-based inference automatically.

------------------------------------------------------------------------

## Shared Infrastructure: Bootstrap & Standardization {.smaller}

### Features Available Across All Engines

**Bootstrap Variance Estimation** (`shared/bootstrap.R`)

```{r}
#| eval: false
#| echo: true
# Works with any engine
engine <- el_engine(
  variance_method = "bootstrap",
  bootstrap_reps = 500
)
```

-   **IID Data:** Nonparametric bootstrap (resamples respondents with replacement).
-   **Survey Data:** Uses survey replicate weights if provided
-   **Robust:** Works even when analytic (delta) method fails (e.g., with weight trimming).

**Predictor Standardization** (`shared/scaling.R`)

```{r}
#| eval: false
#| echo: true
engine <- el_engine(standardize = TRUE) # Default
```

-   **Why:** Improves numerical conditioning of the Jacobian matrix, aiding solver convergence.
-   **What:** All design matrices are centered and scaled.
-   **Transparent:** Parameters are automatically back-transformed to their original scale for reporting.

------------------------------------------------------------------------

## Workflow: From Data to Results {.smaller}

### Step-by-Step Processing Pipeline

::: {style="font-size: 28px;"}
**1. Input Parsing** (`parse_nmar_spec`)

```{r, eval=FALSE, echo=TRUE}
# // Extract outcome, auxiliaries, response predictors from formula
```

**2. Validation** (`validate_nmar_args`)

```{r, eval=FALSE, echo=TRUE}
# // Check: missing values, variable names, engine compatibility
```

**3. Task Creation** (`new_nmar_task`)

```{r, eval=FALSE, echo=TRUE}
# // Bundle validated inputs for the engine
```

**4. Engine Dispatch** (`run_engine.*`)

```{r, eval=FALSE, echo=TRUE}
# // S3 method dispatch based on engine class
```

**5. Data Preparation** (`prepare_nmar_design`)

```{r, eval=FALSE, echo=TRUE}
# // Build design matrices, apply scaling, compute weights
```

**6. Estimation** (Engine-specific, e.g., `el_estimator_core`)

```{r, eval=FALSE, echo=TRUE}
# // Solve the core estimating equations
```

**7. Variance Calculation** (`el_variance_delta` or `bootstrap_variance`)

```{r, eval=FALSE, echo=TRUE}
# // Analytical sandwich or bootstrap resampling
```

**8. Result Object Creation** (`new_nmar_result_*`)

```{r, eval=FALSE, echo=TRUE}
# // Standardized output with print/summary/coef/vcov methods
```
:::

------------------------------------------------------------------------

## S3 Methods and Tidy Integration {.smaller}

### Familiar R Interface, Professional Output

**Standard S3 methods provide a familiar user experience:**

```{r}
#| eval: false
#| echo: true
fit <- nmar(Y_miss ~ X, data = df, engine = el_engine(...))

print(fit) # Compact summary view
summary(fit) # Detailed diagnostics and coefficients
coef(fit) # Extract response model coefficients
vcov(fit) # Variance-covariance matrix of coefficients
confint(fit) # Confidence intervals for the mean estimate
```

**Tidy integration via `broom` is supported:**

```{r}
#| eval: false
#| echo: true
library(broom)
tidy(fit) # Coefficients as a tibble
glance(fit) # Fit statistics as a tibble
```

**The result object contains all relevant information:**

```         
nmar_result
â”œâ”€ y_hat             # Point estimate of E[Y]
â”œâ”€ se                # Standard error of the mean estimate
â”œâ”€ model             # List with $coefficients and $vcov
â”œâ”€ converged         # Logical convergence flag
â”œâ”€ diagnostics       # Rich list of solver and weight diagnostics
â””â”€ meta              # Method, formula, sample sizes
```

## Exponential Tilting Estimator: From Theory to Practice

### A Propensity-score-adjustment Method For Nonignorable Nonresponse (Aka `Exponential Tilting`)

#### Minsun Kim Riddles<br> Jae Kwang Kim<br> Jongho Im

<hr>

<br><br><br><br>

-   Author of this paper gained PhD for their solution
-   I will highlight **overview concept** rather than go in details
-   It took me few weeks to understand it, thus I am not able to explain this in 3 minutes ðŸ¤«

<br>

<br>

## Method Steps

::: {style="font-size: 30px;"}
### 1. Estimate Observed Distribution

> $$
> \hat{f}_1(y) = f(y | X_1, \delta = 1)
> $$

### 2. Compute Score Components

> $S_{\text{obs}} = \sum_{j:\delta_j=1} s(y_j, X_{1j})$, \quad $S_{\text{mis}} = \sum_{j:\delta_j=1} \sum_{i:\delta_i=0} w_{ij} \cdot s(y_j, X_{1i})$

where $w_{ij}$ depends on $\hat{f}_1$ and odds ratio

### 3. Solve Final Equation

> $$
> S_{\text{total}} = S_{\text{obs}} + S_{\text{mis}} = 0
> $$

### 4. Estimate Population Mean

> $$
> \mathbb{E}[Y] = \hat{\theta} = \frac{\sum_{i:\delta_i=1} \frac{1}{\pi_i(\hat{\boldsymbol{\phi}}_p)} y_i}{\sum_{i:\delta_i=1} \frac{1}{\pi_i(\hat{\boldsymbol{\phi}}_p)}}
> $$

### Key Insight:

We never fill in missing values - we reweight observed data to represent the entire population!
:::

------------------------------------------------------------------------

## Empirical Likelihood (EL): Theory Overview

### Based on Qin, Leung &amp; Shao (JASA, 2002)

**Goal.** Estimate the population mean of $Y$ with **nonignorable nonresponse** $R$.

- **Model only the response mechanism**
  
  $$
  w(y,x;\beta)=P(R=1\mid Y{=}y, X{=}x)
  \quad\text{(eg. logit or probit)}
  $$

- **Do not model** $F(y,x)$, treat it **nonparametrically**.

Thus EL is **semiparametric**: finite-dimensional $\beta$ for $w$, nonparametric $F$.

---

## Observedâ€“Data Semiparametric Likelihood

Let $n$ be respondents from a nominal population of size $N$. Define

$$
W \;=\; \iint w(y,x;\beta)\,dF(y,x).
$$

<div class="eq-card">
$$
L(\beta,F)\ \propto\
\Bigg\{\prod_{i:\,R_i=1} w(y_i,x_i;\beta)\,dF(y_i,x_i)\Bigg\}\,
W^{\,n}\,(1-W)^{\,N-n}.
$$
</div>

We **profile out $F$** using the empirical likelihood framework.

---

## Profiling Out $F$: EL Masses and Constraints

Place masses $p_i$ on respondents $(y_i,x_i)$ and, **for fixed $(\beta,W)$**, maximize the $F$-part of the likelihood subject to:

- **Probability:** $\sum p_i=1$
- **Response-rate calibration:** $\sum p_i\{w(y_i,x_i;\beta)-W\}=0$
- **Auxiliary calibration (optional):** $\sum p_i (x_i-\mu_x)=0$  
  (Use known $\mu_x$ or $\bar X$; drop if no auxiliaries.)

Inner profiling objective:

::: columns
::: column
**IID:** $\displaystyle \max_{\{p_i\}} \sum_{i:R_i=1}\log p_i$
:::
::: column
**Survey designs:** $\displaystyle \max_{\{p_i\}} \sum_{i:R_i=1} a_i\,\log p_i$
:::
:::

Lagrange multipliers $(\lambda_W,\lambda_x)$ yield the EL denominator
<div class="eq-card">
$$
D_i \;=\; 1 + \lambda_W\big(w(y_i,x_i;\beta)-W\big) + (x_i-\mu_x)^\top\lambda_x.
$$
</div>

---

## Closed-Form EL Weights and $\lambda_W$

**i.i.d. (SRS):**
$$
p_i \;=\; \frac{1}{n\,D_i}.
$$

**Survey designs (base weights $a_i$):**
$$
p_i^{\text{EL}}\ \propto\ \frac{a_i}{D_i}
\quad\text{(normalization cancels in }\ \hat\mu_Y=\tfrac{\sum p_i^{\text{EL}}y_i}{\sum p_i^{\text{EL}}}\text{)}.
$$

**Key identity (derived, not free):**
$$
\lambda_W \;=\; \frac{\dfrac{\sum_{\text{all}} a_i}{\sum_{R=1} a_i}\,-\,1}{1-W}
\quad\text{(SRS: } \lambda_W=\tfrac{N/n-1}{1-W}\text{)}.
$$

---

## Estimating Equations (Stacked System)

Let $Z_i$ be the response design row, $\eta_i=Z_i\beta$, $w_i=\operatorname{linkinv}(\eta_i)$,
$\mu_{\eta,i}=\dfrac{dw}{d\eta}(\eta_i)$, and $s_i=\mu_{\eta,i}/w_i$.
With design weights $a_i$ (set $a_i=1$ for i.i.d.):

<div class="smallish">

- **Auxiliary calibration** ($L$ eqns; omit if no auxiliaries):
  $$
  \sum_i a_i\,\frac{x_i-\mu_x}{D_i} \;=\; 0
  $$

- **Response rate**:
  $$
  \sum_i a_i\,\frac{w_i - W}{D_i} \;=\; 0
  $$

- **Response-model score balance** ($K$ eqns):
  $$
  \sum_i a_i\, Z_i \Bigg[s_i\;-\;\frac{\lambda_W\,\mu_{\eta,i}}{D_i}\Bigg] \;=\; 0
  $$

where $D_i=1+\lambda_W(w_i-W)+(x_i-\mu_x)^\top\lambda_x$.
</div>

We solve for $(\beta, W, \lambda_x)$ with $\lambda_W$ **given by the identity above**. Often optimize on $z=\operatorname{logit}(W)$.

---

## Final Estimator

Using EL weights:
<div class="eq-card">
$$
\hat\mu_Y \;=\; \frac{\sum_{i:R_i=1} p_i^{\text{EL}}\,y_i}{\sum_{i:R_i=1} p_i^{\text{EL}}}
\quad \text{with}\quad
p_i^{\text{EL}} \propto \begin{cases}
\dfrac{1}{D_i}, & \text{i.i.d.}\\[6pt]
\dfrac{a_i}{D_i}, & \text{survey}
\end{cases}
$$
</div>

**Consistency.** If $w(y,x;\beta)$ is correctly specified and standard regularity holds, $\hat\mu_Y \xrightarrow{p} \mu_Y$.

**Asymptotic normality.** 
$$
\sqrt{N}\,(\hat\mu_Y-\mu_Y)\ \overset{d}{\longrightarrow}\ N(0,\sigma^2),
$$
with $\sigma^2$ depending on the auxiliary-info case (A/B/C) and the design.

**Bias.** Asymptotically unbiased; small-sample bias can occur with extreme weights/high nonresponse. Auxiliary calibration (A/B) typically **reduces variance** (and improves stability). If trimming is used, expect biasâ€“variance tradeoffs; prefer bootstrap SEs.

---

## From Theory to Code: The `nmar` Implementation {.smaller}

### How we solve the system robustly

The theoretical equations are elegant but numerically challenging. Our implementation is hardened for reliable inference.

-   **Challenge 1: Parameter Bounds** ($W \in (0,1)$)
    -   **Solution:** We solve for $z = \text{logit}(W) \in \mathbb{R}$, an unconstrained reparameterization.
-   **Challenge 2: Solver Performance** (Slow/unstable convergence)
    -   **Solution:** We use an **analytical Jacobian** and a **staged solver** (Newton -\> Perturbed Restarts -\> Broyden).
-   **Challenge 3: Numerical Stability** (Division by zero, `log(0)`)
    -   **Solution:** We apply **numerical safeguards** (denominator guards, eta capping) at every step.
-   **Challenge 4: Survey Data** (IID assumption violation)
    -   **Solution:** We let design weights enter into estimating equations
-   **Challenge 5: Extreme el weights**
    -   **Solution:** Trimming, diagnostics, numerical guards

---

## Implementation Detail: Reparameterization {.smaller}

### Ensuring Valid Response Rates: $W \to z = \text{logit}(W)$

**The Challenge:** - The paper's parameterization uses $W \in (0,1)$ directly. - A standard Newton solver could propose invalid updates, e.g., $W > 1$.

**Our Solution:** We optimize over the unconstrained logit-transformed rate $z$.

```{r}
#| eval: false
#| echo: true
# Internally, the solver sees `z`
z <- qlogis(W)

# We recover W, which is always valid, via the inverse transform
W <- plogis(z)
```

**Benefits:** 

- The optimization is unconstrained, making the solver's job easier. 
- The Jacobian is adjusted for the transformation via the chain rule. 
- Boundary violations are impossible. - Optimization is numerically stable even as $W$ approaches 0 or 1.

**Implementation:** Internal EL functions operate in the $(\beta, z, \lambda_x)$ space.

---

## Implementation Detail: Numerical Safeguards {.smaller}

**1. Eta Capping** (`get_eta_cap()`)

```{r, eval=FALSE, echo=TRUE}
# // Prevent extreme linear predictor values, which cause Inf/NaN in link functions
eta_i <- pmax(pmin(eta_raw, 50), -50)
```

**2. Probability Bounding**

```{r, eval=FALSE, echo=TRUE}
# // Keep response probabilities away from exact 0 or 1
w_i <- pmin(pmax(w_i, 1e-12), 1 - 1e-12)
```

**3. Denominator Guarding**

```{r, eval=FALSE, echo=TRUE}
# // The EL denominator defines the weights and MUST be positive.
denominator <- 1 + lambda_W * (w_i - W_bounded)
if (K_aux > 0) denominator <- denominator + as.vector(X_centered %*% lambda_x)

inv_denominator <- 1 / pmax(denominator, 1e-8)
```

-   This guard is applied in the estimating equations, the Jacobian, and the final weight calculation, preventing division by zero and ensuring positive EL weights.

**Result:** The estimator converges reliably even in challenging scenarios where naive implementations would fail.

---

## Implementation Detail: Analytic Jacobian {.smaller}

### Exact Derivatives for a Fast and Accurate Newton Solver

**Why bother with Analytic?**

-   **Quadratic Convergence:** Newton's method with an exact Jacobian converges extremely quickly near the solution.

-   **Accuracy:** Avoids approximation errors inherent in numerical derivatives.

-   **Robustness:** Not sensitive to the step size choices required by numeric differentiation.

$$ 
A = \begin{pmatrix}
\frac{\partial F_\beta}{\partial \beta} & \frac{\partial F_\beta}{\partial z} & \frac{\partial F_\beta}{\partial \lambda_x} \\
\frac{\partial F_W}{\partial \beta} & \frac{\partial F_W}{\partial z} & \frac{\partial F_W}{\partial \lambda_x} \\
\frac{\partial F_\lambda}{\partial \beta} & \frac{\partial F_\lambda}{\partial z} & \frac{\partial F_\lambda}{\partial \lambda_x}
\end{pmatrix} 
$$

---

## Variance Estimation: Two Approaches {.smaller}

### Analytical Delta Method vs. Bootstrap

The `nmar` package offers two variance estimation methods, chosen via `variance_method = ...`.

. . .

**1. Delta Method** (`"delta"`)

Uses the analytic sandwich formula.

**Pros:** Fast, deterministic. Asymptotic normality.

**Cons:** May be numerically fragile if $A$ is ill-conditioned; not valid with weight trimming. Notoriously difficult to implement.

. . .

**2. Bootstrap** (`"bootstrap"`)

Performs resampling to estimate variance empirically. 

- **IID:** Resamples respondents with replacement. 
- **Survey:** Uses replicate weights from the survey design.

**Pros:** Robust in practice, valid with trimming, empirically validated. 
**Cons:** Computationally intensive.

**Recommendation:** For now, use `"bootstrap"` for final results, `"delta"` for exploration.

---

## Implementation Detail: Survey Design Support {.smaller}

### Complex Sampling and Design-Based Inference

The package automatically distinguishes between IID and complex survey data.

```{r}
#| eval: false
#| echo: true
# IID data (data.frame)
fit_iid <- nmar(Y_miss ~ X, data = df, engine = el_engine(...))

# Complex survey design (survey.design)
library(survey)
des <- svydesign(ids = ~PSU, strata = ~stratum, weights = ~wt, data = df)
fit_svy <- nmar(Y_miss ~ X, data = des, engine = el_engine(...))
```

**Design-based features implemented:** - **Weighted Equations:** The design weights $a_i$ (from `weights(des)`) are correctly incorporated into the estimating system.

**Design-based Score Variance:** The "meat" matrix $B$ is computed via `survey::svytotal()` and `vcov()`, which correctly accounts for stratification, clustering, and unequal probabilities.

**Implementation:** Separate S3 methods (`el.data.frame` vs `el.survey.design`) dispatch to the correct variance calculation.

---

## Diagnostics & Introspection {.smaller}

A rich set of diagnostics is crucial for trusting NMAR models.

**Accessing diagnostics:**

```{r, eval=FALSE, echo=TRUE}
fit <- nmar(...)
summary(fit)
# or for the full list:
diagnostics <- nmar_result_get_diagnostics(fit)
```

**Key diagnostics and what they mean:** 

- `jacobian_condition_number`: Measures stability of the system. 
- `max_equation_residual`: Should be near zero if converged. 
- `min_denominator`: Must be positive. Values near zero suggest unstable weights. 
- `ess` (Effective Sample Size of Weights): Measures weight degeneracy. 
- `max_weight_share`: The influence of the single most important observation. 
- `trim_fraction`: The proportion of observations whose weights were trimmed.

**Interpretation Guide:** 

- `ess < 30` or `max_weight_share > 0.3`: Results depend on a few data points. Re-check your model or add auxiliary variables. 
- `kappa > 1e10`: The Jacobian is ill-conditioned. Trust bootstrap variance over delta method. 
- `trim_fraction > 0.1`: A large proportion of weights were trimmed. The model may be misspecified, and the estimate is likely biased.

------------------------------------------------------------------------

## NMAR Package in Action

```{r}
#| echo: false
#| message: false
#| warning: false
devtools::load_all()
```

```{r}
#| echo: false
#| message: false
# Function to generate testdata, basing on riddle(2016)
generate_test_data <- function(n_rows = 500, n_cols = 1, case = 1, x_var = 0.5, eps_var = 0.9, a = 0.8, b = -0.2) {
# Generate X variables - fixed to match comparison
  X <- as.data.frame(replicate(n_cols, rnorm(n_rows, 0, sqrt(x_var))))
  colnames(X) <- paste0("x", 1:n_cols)

# Generate Y - fixed coefficients to match comparison
  eps <- rnorm(n_rows, 0, sqrt(eps_var))
  if (case == 1) {
# Use fixed coefficient of 1 for all x variables to match: y = -1 + x1 + epsilon
    X$Y <- as.vector(-1 + as.matrix(X) %*% rep(1, n_cols) + eps)
  }
  else if (case == 2) {
    X$Y <- -2 + 0.5 * exp(as.matrix(X) %*% rep(1, n_cols)) + eps
  }
  else if (case == 3) {
    X$Y <- -1 + sin(2 * as.matrix(X) %*% rep(1, n_cols)) + eps
  }
  else if (case == 4) {
    X$Y <- -1 + 0.4 * as.matrix(X)^3 %*% rep(1, n_cols) + eps
  }

  Y_original <- X$Y

# Missingness mechanism - identical to comparison
  pi_obs <- 1 / (1 + exp(-(a + b * X$Y)))

# Create missing values
  mask <- runif(nrow(X)) > pi_obs
  mask[1] <- FALSE # Ensure at least one observation is not missing
  X$Y[mask] <- NA

  return(list(X = X, Y_original = Y_original))
}

set.seed(1109)
res_test_data <- generate_test_data(n_rows = 500, n_cols = 2, case = 1)
x <- res_test_data$X
y_original <- res_test_data$Y_original # to compare with true values
```

```{r}
#| echo: true
#| message: false
#| warning: false
exptilt_config <- exptilt_engine(
  y_dens = 'normal',
  min_iter = 3,
  max_iter = 10,
  tol_value = 0.01,
  standardize = F,
  family = 'logit',
  bootstrap_reps = 20,
  variance_method = 'bootstrap'
)
formula = Y ~ x1 + x2
res <- nmar(formula = formula, data = x, engine = exptilt_config, response_predictors = NULL)
```

```{r}
#| echo: true
#| message: false
#| warning: false
print(res)
```

```{r}
#| echo: true
#| message: false
#| warning: false
coef(res)
```

```{r}
#| echo: false
#| message: false
#| warning: false
cat('True Y mean:          ', sprintf('%.4f', mean(y_original)), '\n')
est <- as.numeric(res$y_hat)
se <- res$se
cat('Est Y mean (NMAR):    ', sprintf('%.4f', est),
    '  3Ïƒ interval: (', sprintf('%.4f', est - 1.5 * se),
    ', ', sprintf('%.4f', est + 1.5 * se), 'Ïƒ=', sprintf('%.4f', se), ')\n')
cat('Naive Y mean (MAR):   ', sprintf('%.4f', mean(x[!is.na(x$Y), 'Y'])), '\n')
```

## Future Work and Challenges

-   Large-scale data handling
-   Further numerical performance and stability tweaks
-   Analytical variance calculation
-   Bootstrap parallelization
-   Extension to new estimation approaches

## Discussion and Q&A

## References
- Qin, Leung, Shao, 2002
- Kim & Riddles 2012
- Riddles, Kim & Im 2016
