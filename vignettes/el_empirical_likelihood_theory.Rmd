---
title: "Empirical Likelihood Theory"
description: >
  Mathematical details behind the empirical likelihood estimator for NMAR data (Qin, Leung, and Shao 2002), and how the theory maps to our implementation.
output: rmarkdown::html_document
vignette: >
  %\VignetteIndexEntry{Empirical Likelihood Theory for NMAR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This document explains every mathematical object, equation, and derivation behind the empirical likelihood (EL) estimator implemented in the `nmar` package, and maps each concept to code. It covers both data-frame (IID) and survey design use cases, handles arbitrary numbers of response-model and auxiliary covariates, and supports both logit and probit response families.

## Notation

### Units
- $i = 1, \ldots, n$ index respondents (those with observed $Y$)
- $R_i \in \{0, 1\}$ is the response indicator; we work on observed subset $R_i = 1$

### Data
- **Outcome**: $Y_i$ (observed when $R_i = 1$; missing otherwise)
- **Response covariates**: row vector $Z_i \in \mathbb{R}^K$, from `model.matrix` of the response RHS
- **Auxiliary covariates**: row vector $X_i \in \mathbb{R}^L$ (possibly $L = 0$), from auxiliary RHS (no intercept)
- **Population auxiliary means**: $\mu_x \in \mathbb{R}^L$, known; names match columns of $X$

### Response Model (Family functions)
- **Linear predictor**: $\eta_i = Z_i \, \beta$
- **Response probability**: $w_i \equiv g(\eta_i) = \mathrm{linkinv}(\eta_i)$
- **First derivative**: $\dfrac{dw}{d\eta}(\eta_i) = \mu_{\eta,i} = \mathrm{mu.eta}(\eta_i)$
- **Second derivative**: $\dfrac{d^2 w}{d\eta^2}(\eta_i) = \mathrm{d2mu.deta2}(\eta_i)$
Here `linkinv`, `mu.eta`, and `d2mu.deta2` refer to the chosen response family (logit or probit). We follow the paper's $w_i$ notation for the response probability and reserve $p_i^{\text{EL}}$ for empirical-likelihood weights.

### Weight Re-parameterization
- $W \in (0,1)$ nuisance scalar; we parameterize via $z = \text{logit}(W)$ for stability and set $W = \text{plogis}(z)$
- $\lambda_W \in \mathbb{R}$ and $\lambda_x \in \mathbb{R}^L$ are EL Lagrange multipliers for constraints; collected together in $\theta$

### EL Weights
- **Denominator**: $D_i = 1 + \lambda_W (w_i - W) + (X_i - \mu_x)^T \lambda_x$
- **Base sampling weights**: $a_i = 1$ (IID) or $a_i =$ survey base weight for respondent $i$
- **EL weights for respondents**: $p_i^{\text{EL}} \propto a_i / D_i$ (proportionality normalized by totals below)

### Estimator
- $\hat{Y} = \sum p_i^{\text{EL}} Y_i / \sum p_i^{\text{EL}}$

### Notation at a Glance

| Symbol | Meaning |
|---|---|
| $i$ | Respondent index (rows with observed $Y$) |
| $Y_i$ | Outcome for unit $i$ (observed if $R_i=1$) |
| $Z_i$ | Row of response design matrix (includes intercept) |
| $X_i$ | Row of auxiliary design (no intercept) |
| $\mu_x$ | Known population means of auxiliaries (vector) |
| $\beta$ | Response-model coefficients |
| $\eta_i=Z_i\beta$ | Linear predictor for response model |
| $w_i$ | $\mathrm{linkinv}(\eta_i)$ (logit: $\mathrm{plogis}$; probit: $\Phi$) |
| $\mu_{\eta,i}$ | $\dfrac{dw_i}{d\eta_i}$ |
| $\lambda_W$ | Multiplier for the $W$-constraint $\sum (w_i-W)/D_i=0$ |
| $\lambda_x$ | Multipliers for auxiliary constraints $\sum (X_i-\mu_x)/D_i=0$ |
| $D_i$ | $1+\lambda_W(w_i-W)+(X_i-\mu_x)^T\lambda_x$ |
| $a_i$ | Base weight (IID: 1; survey: design weight) |
| $p_i^{\mathrm{EL}}$ | Empirical-likelihood weight $\propto a_i/D_i$ |
| $\hat Y$ | $\sum p_i^{\mathrm{EL}} Y_i/\sum p_i^{\mathrm{EL}}$ |
| $F(\theta)$ | Estimating system (beta, W, and auxiliary equations) |
| $A$ | Jacobian $\partial F/\partial \theta$ |
| $B$ | Covariance of total score vector |
| $g(\theta)$ | Functional mapping parameters to $\hat Y$ |
| $\mathrm{Var}(\hat Y)$ | Delta: $\nabla g\,A^{-1}BA^{-T}\,\nabla g^T$ |

### Engines
- Family: "logit" (default) or "probit"; both use the log-likelihood score w.r.t. $\eta$: $s_i = \partial\log w_i/\partial\eta_i = \mu_{\eta,i}/w_i$ (for respondents, $\delta_i=1$)
- Scaling: optional standardization of design matrices and $\mu_x$ via nmar_scaling_recipe

## From Paper to Implementation: Core Ideas

The paper (Qin-Leung-Shao, JASA 2002) sets EL under nonignorable response using:

- **Empirical likelihood weights** for respondents that satisfy:
  - Zero-sum residual: $\sum p_i^{\text{EL}} (w_i - W) = 0$
  - Auxiliary moments: $\sum p_i^{\text{EL}} (X_i - \mu_x) = 0$
- A **response model probability** $w_i = g(\eta_i)$, $\eta_i = Z_i \, \beta$

In our code, we adopt the same EL structure and estimating equations. We extend it to arbitrary $Z$ and $X$, and to survey designs via design-based covariance for variance estimation. Because $\hat Y$ is a ratio-of-weights estimator, any common normalization of $p_i^{\text{EL}} \propto a_i/D_i$ cancels in $\hat Y$; only relative weights matter (the KKT multipliers $\lambda$ enforce the constraints; normalization affects only a common scale that vanishes in the ratio).

### Equation Crosswalk (QLS 2002 -> This Vignette/Code)

- QLS (5): Discrete mass form for $p_i$ with two multipliers -> Our $D_i = 1 + \lambda_W (w_i - W) + (X_i - \mu_x)^T \lambda_x$ and $p_i^{\text{EL}} \propto a_i/D_i$.
- QLS (7): $\sum \dfrac{x_i - \bar x}{1 + \cdots} = 0$ -> Our auxiliary constraints $\sum a_i (X_i - \mu_x)/D_i = 0$.
- QLS (8): $\sum \dfrac{w_i - W}{1 + \cdots} = 0$ -> Our $W$-equation $\sum a_i (w_i - W)/D_i = 0$.
- QLS (10): $\hat{\lambda}_2 = (N/n - 1)/(1 - W)$ -> Our $\lambda_W = ((N_{\text{pop}}/n_{\text{resp\_weighted}}) - 1)/(1 - W)$ (design-weighted generalization).
- Estimator $\hat Y$ in QLS -> Our ratio $\hat Y = \sum p_i^{\mathrm{EL}} Y_i/\sum p_i^{\mathrm{EL}}$ using $p_i^{\mathrm{EL}} \propto a_i/D_i$.

### Likelihood and Profiling (sketch)

The paper's semiparametric likelihood (their Eq. (2)) combines the response mechanism $w_i = g(\eta_i)$ with the nonparametric distribution $F$ of $(Y,X)$:

\[
\mathcal{L}(\beta, W, F) 
\;\propto\; \prod_{i=1}^{n} w(Y_i, X_i; \beta) \, dF(Y_i, X_i) 
\;\times\; (1 - W)^{N - n},
\]

subject to (i) $\int dF = 1$, (ii) $\int X \, dF = \mu_x$, and (iii) $\int w(Y,X;\beta) \, dF = W$. Discretizing $F$ at observed respondents by assigning unknown masses $p_i$ and introducing multipliers $\lambda$, the KKT conditions yield the familiar EL weight form with denominator

\[
D_i \;=\; 1 + \lambda_W (w_i - W) + (X_i - \mu_x)^\top \lambda_x,
\]

and, with base weights $a_i$, the working weights are $p_i^{\text{EL}} \propto a_i / D_i$.

Remark on conditioning: QLS's Eq. (2) writes the first product as $\prod_i [\, w(y_i,x_i;\beta)\,dF(y_i,x_i)/W\,]$ so that it explicitly represents the likelihood of $(Y_i,X_i)$ conditional on $R_i=1$. Multiplying by the binomial term $W^n(1-W)^{N-n}$ yields the same overall likelihood as above because the $W^{-n}$ in the first factor cancels the $W^n$ in the second. Both factorizations lead to the same estimating equations and the same profiled log-likelihood form used subsequently in QLS after introducing the multipliers.

### KKT and Denominator (details)

Introducing Lagrange multipliers $(\lambda_0,\lambda_x,\lambda_W)$ for these constraints and profiling the $p_i$'s gives the KKT stationarity conditions

\[
\frac{\partial}{\partial p_i} \Big[ \sum_j \log(a_j p_j) - \lambda_0 (\sum_j p_j - 1) - \lambda_x^T \sum_j p_j (X_j - \mu_x) - \lambda_W \sum_j p_j (w_j - W) \Big] = 0,
\]

which solve to

\[
p_i \;\propto\; \frac{1}{\,1 + \lambda_x^T (X_i-\mu_x) + \lambda_W (w_i - W)\,} \;\equiv\; \frac{1}{D_i}.
\]

Normalizing to enforce $\sum p_i = 1$ yields $p_i = \frac{D_i^{-1}}{\sum_j D_j^{-1}}$. In the presence of base sampling weights $a_i$ (survey designs), the same derivation gives the natural generalization

\[
p_i^{\text{EL}} \;\propto\; \frac{a_i}{D_i} \quad \text{with} \quad D_i = 1 + \lambda_W (w_i - W) + (X_i - \mu_x)^T\lambda_x.
\]

This is exactly the working form used in our estimator. The EL weights $p_i^{\text{EL}}$ are then used to build the mean estimator

\[
\hat Y \;=\; \frac{\sum_i p_i^{\text{EL}} Y_i}{\sum_i p_i^{\text{EL}}}.
\]

The remaining unknowns $(\beta, W, \lambda_x)$ are determined by the estimating equations below.

### Clarification: Relationship Between $W$ and $\lambda_W$

In our reparameterization, the EL multiplier for the probability constraint is expressed as:

\[\lambda_W = \frac{C}{1 - W}, \quad \text{with } C = \frac{N_{\text{pop}}}{n_{\text{resp\_weighted}}} - 1 \text{ and } W = \text{plogis}(z)\]

Intuition: In the EL KKT system, the constraint $\sum p_i^{\text{EL}} (w_i - W) = 0$ sits alongside normalization and (optionally) auxiliary constraints. Incorporating base weights $a_i$ and the ratio between population and respondent totals induces a scaling of the multiplier linked to the mass constraint. Writing $\lambda_W$ in this scaled form keeps the parameter on a numerically stable scale and lets the derivative structure (w.r.t. $z$ via $W$) be handled cleanly. This is consistent with the EL structure when the baseline mass is $n_{\text{resp\_weighted}}$ and the "full population" target is $N_{\text{pop}}$, and it is exactly what the code uses to match the normalization implied by base weights.

Derivation sketch (KKT): The discretized semiparametric likelihood (QLS, 2002) maximizes, over the unknown masses $\{p_i\}$ at observed points and over $(\beta, W)$,

\[
\ell(\beta, W, \lambda_x, \lambda_W) 
\;=\; \sum_{i=1}^{n} \log w_i(\beta)
\; +\; (N_{\text{pop}} - n_{\text{resp\_weighted}}) \log(1 - W)
\; -\; \sum_{i=1}^{n} \log\!\Big(1 + (X_i - \mu_x)^\top \lambda_x + \lambda_W (w_i - W)\Big),
\]

subject to the normalization and moment constraints that generate the EL denominator. For the weighted-EL variant we work with unnormalized respondent weights proportional to $a_i/D_i$; choosing the conventional normalization

\[
\sum_{i=1}^{n} \frac{a_i}{D_i} \,=\, n_{\text{resp\_weighted}} \equiv \sum_{i=1}^{n} a_i
\]

recovers the same estimating system (and any common normalization cancels in the ratio estimator $\hat Y = \sum p_i Y_i/\sum p_i$). Taking derivatives (KKT conditions) and using that $\partial/\partial W$ of the second and third terms produces opposing contributions, one obtains the system equivalent to QLS (7)-(10). In particular, the first-order condition w.r.t. the multiplier associated with the $W$-constraint yields, together with the derivative w.r.t. $W$, the closed form

\[
\lambda_W 
\;=\; \frac{\tfrac{N_{\text{pop}}}{n_{\text{resp\_weighted}}} - 1}{1 - W}
\;=\; \frac{C}{1 - W},
\]

which generalizes QLS (10) to the design-weighted case ($n_{\text{resp\_weighted}} = \sum_i a_i$ and $N_{\text{pop}} = \sum_{\text{all}} a_i$). A quick derivation sketch mirrors QLS (7)-(10): the W-FOC yields $\sum_i a_i (w_i - W) / D_i = 0$; combining with the implicit normalization $\sum_i a_i / D_i = n_{\text{resp\_weighted}}$ and the binomial part for $W$ gives $\lambda_W (1-W)\, n_{\text{resp\_weighted}} = N_{\text{pop}} - n_{\text{resp\_weighted}}$, hence $\lambda_W = (N_{\text{pop}}/n_{\text{resp\_weighted}} - 1)/(1-W)$. This is the expression implemented in the estimator and used throughout the Jacobian and variance calculations.

## Estimating Equations

**Unknown parameters**: $\beta \in \mathbb{R}^K$, $z \in \mathbb{R}$ (for $W = \text{plogis}(z)$), $\lambda_x \in \mathbb{R}^L$; define $\theta = (\beta, z, \lambda_x)$.

Define $w_i = \mathrm{linkinv}(\eta_i)$ and $\mu_{\eta,i} = \frac{dw}{d\eta}(\eta_i)$ (denoted `mu.eta(eta_i)` in code).

Define $C = \frac{N_{\text{pop}}}{n_{\text{resp\_weighted}}} - 1$, with $n_{\text{resp\_weighted}} = \sum a_i$ and $N_{\text{pop}}$ known (sum of design weights for survey; $n$ for IID). Then $\lambda_W = \frac{C}{1 - W}$.

**Denominator**: $D_i = 1 + \lambda_W (w_i - W) + (X_i - \mu_x)^T \lambda_x$, with $D_i \geq \epsilon$ enforced numerically.

Define the score term $s_i = \mu_{\eta,i}/w_i$ (the unit-level contribution to the log-likelihood score with respect to $\eta$). For logit, $s_i = 1 - w_i$; for probit, $s_i = \phi(\eta_i)/\Phi(\eta_i)$ (computed stably in code).

Intuition (why this score appears): for each respondent we observe $R_i=1$, so the Bernoulli log-likelihood contribution of the response model is $\log w_i(\eta_i)$. Differentiating w.r.t. the linear predictor gives
\[
\frac{\partial}{\partial\eta_i} \log w_i(\eta_i) \,=\, \frac{1}{w_i}\, \frac{dw_i}{d\eta_i} \,=\, \frac{\mu_{\eta,i}}{w_i} \;\equiv\; s_i.
\]

Thus $s_i$ measures the local sensitivity of the observed-response likelihood to $\eta_i$. In the logit family, $\mu_{\eta,i}=w_i(1-w_i)$ so $s_i=1-w_i$-the familiar residual-like term; in the probit family, $s_i=\phi(\eta_i)/\Phi(\eta_i)$, the (inverse) Mills ratio. The EL $\beta$-equations balance this likelihood score against the EL penalty term $\lambda_W\,\mu_{\eta,i}/D_i$, enforcing the calibration constraints while fitting the response model.

### The System of Estimating Equations $F(\theta) = 0$

**$\beta$-equations** ($K$ equations):
$$\sum a_i Z_i [s_i - \lambda_W \mu_{\eta,i} / D_i] = 0$$

**W-equation** (1 equation):
$$\sum a_i (w_i - W) / D_i = 0$$

**Auxiliary constraints** ($L$ equations):
$$\sum a_i (X_i - \mu_x) / D_i = 0$$

These are exactly how `build_equation_system` constructs the function in code (`src_dev/engines/el/impl/equations.R`).

Intuition: the $\beta$-equations equate the score of the respondent log-likelihood with the EL penalty term $\lambda_W \mu_{\eta,i}/D_i$; the $W$-equation centers the modeled response probabilities around the unconditional mean $W$ under the EL weights; the auxiliary equations calibrate the centered auxiliaries to zero mean under the EL weights.

### Remarks

- For logit and probit, $s_i$ is the log-likelihood score $\partial\log w_i/\partial\eta_i = \mu_{\eta,i}/w_i$ (equals $1-w_i$ for logit; $\phi/\Phi$ for probit). This follows the paper's MLE derivation; EL constraints supply the nonparametric part.

## Analytical Jacobian ($A$ Matrix)

We differentiate $F(\theta) = 0$ with respect to $\theta = (\beta, z, \lambda_x)$. Let:

- $\eta_i = Z_i \beta$, $w_i = \text{linkinv}(\eta_i)$, $\mu_{\eta,i} = \dfrac{dw}{d\eta}(\eta_i)$, $\mu''_i = \dfrac{d^2 w}{d\eta^2}(\eta_i)$
- $W = \text{plogis}(z)$, $\frac{dW}{dz} = W(1 - W)$
- $\lambda_W = \frac{C}{1 - W}$, so $\frac{d\lambda_W}{dW} = \frac{C}{(1 - W)^2}$ and $\frac{d\lambda_W}{dz} = \frac{d\lambda_W}{dW} \cdot \frac{dW}{dz}$
- $X_{\text{centered},i} = X_i - \mu_x$

### Intermediate Derivatives

- $s_i = \mu_{\eta,i}/w_i \Rightarrow \;\frac{ds_i}{d\eta_i} = (\mu'_{\eta,i}w_i - \mu_{\eta,i}^2)/w_i^2$ with $\mu'_{\eta,i} = \dfrac{d\mu_{\eta,i}}{d\eta_i} = \dfrac{d^2 w}{d\eta_i^2} \equiv \mu''_i$ (this is `d2mu.deta2(eta_i)` in code)
- $D_i = 1 + \lambda_W (w_i - W) + X_{\text{centered},i}^T \lambda_x$
  - $\frac{\partial D_i}{\partial \eta_i} = \lambda_W \mu_{\eta,i}$
  - $\frac{\partial D_i}{\partial z} = \frac{\partial \lambda_W}{\partial z} \cdot (w_i - W) - \lambda_W \cdot \frac{dW}{dz}$
  - $\frac{\partial D_i}{\partial \lambda_x} = X_{\text{centered},i}$

Define $\text{inv}_i = 1 / D_i$ and the scalar term driving $\beta$-equations:

$$T_i = s_i - \lambda_W \mu_{\eta,i} \text{inv}_i,\quad s_i = \frac{\mu_{\eta,i}}{w_i}.$$

### Compute Its Derivatives

Using $\,\mu'_{\eta,i} = d\mu_{\eta,i}/d\eta_i = \mathrm{d2mu\,deta2}(\eta_i)$ and $\,dw_i/d\eta_i = \mu_{\eta,i}$,

$$\frac{\partial s_i}{\partial \eta_i} = \frac{\mu'_{\eta,i} w_i - \mu_{\eta,i}^2}{w_i^2}.$$

Also $\,\frac{\partial \text{inv}_i}{\partial \eta_i} = -\text{inv}_i^2 \cdot \frac{\partial D_i}{\partial \eta_i} = -\text{inv}_i^2 (\lambda_W \mu_{\eta,i})$. Therefore

$$\frac{\partial T_i}{\partial \eta_i} = \frac{\mu'_{\eta,i} w_i - \mu_{\eta,i}^2}{w_i^2} - \lambda_W \mu'_{\eta,i} \text{inv}_i + \lambda_W^2 (\mu_{\eta,i})^2 \text{inv}_i^2.$$

$$\frac{\partial T_i}{\partial z} = -\frac{\partial \lambda_W}{\partial z} \cdot \mu_{\eta,i} \text{inv}_i + \lambda_W \mu_{\eta,i} \text{inv}_i^2 \cdot \frac{\partial D_i}{\partial z}$$

$$\frac{\partial T_i}{\partial \lambda_x} = \lambda_W \mu_{\eta,i} \text{inv}_i^2 \cdot X_{\text{centered},i}$$

### Assemble Jacobian Blocks (with $a_i$ weights)

**$J_{\beta\beta}$ ($K \times K$)**:
$$J_{11} = \sum a_i Z_i^T \left[ \frac{\partial T_i}{\partial \eta_i} \right] Z_i$$

**$J_{\beta z}$ ($K \times 1$)**:
$$J_{12} = \sum a_i Z_i^T \left[ \frac{\partial T_i}{\partial z} \right]$$

**$J_{\beta \lambda}$ ($K \times L$)**:
$$J_{13} = \sum a_i Z_i^T \left[ \frac{\partial T_i}{\partial \lambda_x} \right]$$

**$J_{z\beta}$ ($1 \times K$)**: derivative of W-equation w.r.t. $\beta$

Equation: $G_W = \sum a_i (w_i - W) \text{inv}_i$

$$\frac{\partial G_W}{\partial \eta_i} = a_i \left[ \mu_{\eta,i} \text{inv}_i - (w_i - W) \text{inv}_i^2 \left(\frac{\partial D_i}{\partial \eta_i}\right) \right] = a_i \left[ \mu_{\eta,i} \text{inv}_i - (w_i - W) \text{inv}_i^2 (\lambda_W \mu_{\eta,i}) \right]$$

Then: $J_{21} = \sum \frac{\partial G_W}{\partial \eta_i} \cdot Z_i$

**$J_{zz}$ ($1 \times 1$)**:
$$\frac{\partial G_W}{\partial z} = \sum a_i \left[ -\frac{dW}{dz} \cdot \text{inv}_i - (w_i - W) \text{inv}_i^2 \cdot \frac{\partial D_i}{\partial z} \right]$$

**$J_{z\lambda}$ ($1 \times L$)**:
$$\frac{\partial G_W}{\partial \lambda_x} = \sum a_i \left[ -(w_i - W) \text{inv}_i^2 X_{\text{centered},i} \right]$$

**$J_{\lambda\beta}$ ($L \times K$)**: constraints $H(\lambda): \sum a_i \text{inv}_i X_{\text{centered},i} = 0$

$$\frac{\partial H}{\partial \eta_i} = -a_i \text{inv}_i^2 \frac{\partial D_i}{\partial \eta_i} X_{\text{centered},i} = -a_i \text{inv}_i^2 (\lambda_W \mu_{\eta,i}) X_{\text{centered},i}$$

Thus, component-wise $J_{31} = \sum_i a_i\,(-\lambda_W \mu_{\eta,i}\,\text{inv}_i^2)\, X_{\text{centered},i}^T Z_i$. In compact matrix form:

$$J_{31} = X_{\text{centered}}^T \operatorname{diag}\!\big(-a_i\,\lambda_W\,\mu_{\eta,i}\,\text{inv}_i^2\big) Z.$$

**$J_{\lambda z}$ ($L \times 1$)**:
$$\frac{\partial H}{\partial z} = -\sum a_i \text{inv}_i^2 \left(\frac{\partial D_i}{\partial z}\right) X_{\text{centered},i}$$

**$J_{\lambda\lambda}$ ($L \times L$)**:
$$\frac{\partial H}{\partial \lambda_x} = -X_{\text{centered}}^T \operatorname{diag}(a_i\,\text{inv}_i^2) X_{\text{centered}}.$$

These are exactly what `build_el_jacobian` computes (`src_dev/engines/el/impl/jacobian.R`).

### Why Analytic A Helps

- Newton-Raphson (as used in our outer solve) linearizes $F(\theta)$ near the current iterate: $F(\theta + \Delta) \approx F(\theta) + A(\theta)\,\Delta$. The update $\Delta$ solves $A\,\Delta = -F$, hence a high-quality $A$ is critical for fast, stable convergence.

## Delta Variance: $\nabla g \, A^{-1} \, B \, A^{-T} \, \nabla g^T$

We compute:

- $A = \partial F/\partial\theta$ from the analytic Jacobian when available (logit, probit); numeric otherwise with the same denominator guard.
- $B = \operatorname{Var}(\sum U_i)$ built from the stacked per-respondent contributions $U_i$ (matching $F$), with either i.i.d. covariance or design-based covariance (survey).
- $\nabla g$ evaluated at the solution (analytic when smooth and untrimmed; numeric fallback otherwise).

IID B (data frame path): We compute $\widehat B = \sum (U_i - \bar U)(U_i - \bar U)^T$ at the solution. Centering is asymptotically neutral and reduces small-sample noise.

Survey B: We construct score variables on the full design by placing respondent $U_i$ on observed rows and zeros on nonrespondents, then compute the covariance of `svytotal(~U, design)`. This uses standard survey-inference machinery and respects stratification/clustering/weights.

Trimming: When `trim_cap < Inf`, we recommend `variance_method = "bootstrap"` or strengthen auxiliary constraints. With trimming, the estimator is non-smooth, so bootstrap is recommended for inference.

Implementation: `el_compute_delta_variance()` in `src_dev/engines/el/impl/variance.R` assembles $B$ from `U_matrix_resp` and the provided `compute_score_variance_func` (IID vs survey), computes $\nabla g$ (analytic when smooth; stabilized numeric otherwise), and returns $\mathrm{Var}(\hat{Y})$ and the (scaled) sandwich vcov for $\beta$.

### Worked Example: Analytic $\nabla g$ in a 1-Covariate Logit Case

Consider the simple, smooth setting with no trimming (cap $=\infty$), no auxiliary constraints ($L=0$), and a logit response model
$$
\eta_i = Z_i\,\beta = \beta_0 + \beta_1 x_i,\qquad w_i = \operatorname{logit}^{-1}(\eta_i),\qquad \mu_{\eta,i} = \frac{dw_i}{d\eta_i} = w_i (1-w_i).
$$
Let $C = N_{\text{pop}}/n_{\text{resp\_weighted}} - 1$ and $W = \operatorname{logit}^{-1}(z)$, so
$$
\lambda_W = \frac{C}{1-W},\qquad \frac{dW}{dz} = W(1-W),\qquad \frac{d\lambda_W}{dz} = \frac{C\,W}{(1-W)} = \lambda_W\,\frac{W}{1-W}.
$$
With no auxiliaries, the EL denominator and weights for respondents are
$$
D_i = 1 + \lambda_W (w_i - W),\qquad \pi_i = \frac{a_i}{D_i},\qquad B = \sum_i \pi_i,\qquad \hat Y = g(\theta) = \frac{\sum_i \pi_i Y_i}{B},
$$
where $a_i$ are base weights ($a_i\equiv 1$ for IID; design weights for survey). The analytic gradient of $g$ follows from the ratio rule and $\partial\pi_i/\partial\theta$.

1) Gradient w.r.t. $\beta$.

Since $\partial D_i/\partial\beta = \lambda_W\,\mu_{\eta,i}\,Z_i$, we have $\partial\pi_i/\partial\beta = -\,a_i\,\lambda_W\,\mu_{\eta,i}\,Z_i/ D_i^2$, and therefore
$$
\boxed{\;\frac{\partial g}{\partial \beta} \,=\, \frac{1}{B}\sum_i (Y_i - \hat Y)\,\frac{\partial \pi_i}{\partial\beta}
\;=\; -\,\frac{1}{B}\sum_i (Y_i - \hat Y)\,\frac{a_i\,\lambda_W\,\mu_{\eta,i}}{D_i^2}\,Z_i\; }.
$$

2) Gradient w.r.t. the reparametrized $z = \operatorname{logit}(W)$.

Using $\partial D_i/\partial z = (d\lambda_W/dz)\,(w_i - W) - \lambda_W\,\frac{dW}{dz}$,
$$
\boxed{\;\frac{\partial g}{\partial z} \,=\, \frac{1}{B}\sum_i (Y_i - \hat Y)\,\frac{\partial \pi_i}{\partial z}
\;=\; -\,\frac{1}{B}\sum_i (Y_i - \hat Y)\,\frac{a_i}{D_i^2}\,\Big[\,\frac{d\lambda_W}{dz}\,(w_i - W) - \lambda_W\,\frac{dW}{dz}\,\Big]\; }.
$$

3) (Optional) With auxiliaries $X_i$ and centered $X_{c,i}=X_i-\mu_x$, $D_i = 1 + \lambda_W(w_i - W) + X_{c,i}^\top\lambda_x$ and $\partial D_i/\partial\lambda_x = X_{c,i}$, hence
$$
\boxed{\;\frac{\partial g}{\partial \lambda_x} \,=\, -\,\frac{1}{B}\sum_i (Y_i - \hat Y)\,\frac{a_i}{D_i^2}\,X_{c,i}\; }.
$$

Numerical safeguards. In practice we enforce $D_i \ge \varepsilon$ (e.g., $\varepsilon=10^{-8}$) when forming the gradient and the score contributions used in $\widehat B$. The same guard is used in the estimating equations and in the analytic Jacobian, ensuring consistency between $F(\theta)$, $A=\partial F/\partial\theta$, and $\nabla g(\theta)$.

Code mapping. The implementation uses these expressions when `trim_cap = Inf` (smooth case) inside `el_compute_delta_variance()`; otherwise it falls back to a stabilized numeric gradient. See `src_dev/engines/el/impl/variance.R` for the exact code paths and `src_dev/engines/el/impl/jacobian.R` for the matching $A$.

### Solving Strategy and Initialization

- Unknowns are $\theta = (\beta, z, \lambda_x)$ with $W = \mathrm{plogis}(z)$. We solve the full stacked system $F(\theta) = 0$ via Newton with the analytic Jacobian $A = \partial F/\partial\theta$ using `nleqslv`.
- Globalization and scaling: we rely on `nleqslv`'s globalization (default `global = "dbldog"`) and enforce denominator positivity ($\min_i D_i \ge \varepsilon$) within equation evaluations. Optional standardization of design matrices improves conditioning.
- Initialization: by default $\beta$ starts at zeros in the scaled space (unless the user supplies `start$beta`), and $z$ is seeded at $\mathrm{logit}(\text{observed response rate})$. An internal last-chance Broyden retry may be used if Newton fails to converge; this is not a user-facing mode.

### Variance Assembly and Numerical Stability

Let $A = \frac{\partial F}{\partial \theta}$ be the Jacobian of the estimating system. The delta variance uses the sandwich $\nabla g\,A^{-1} B A^{-T}\,\nabla g^T$. Numerically we avoid forming an explicit inverse:

- Solve $A X = B$ (first linear solve) and then $A^T Y = X^T$ (second linear solve), so that $\Sigma_\theta = A^{-1} B A^{-T} = Y^T$. We lightly symmetrize $\Sigma_\theta$ to reduce round-off.
- If the solves fail or yield non-finite/near-zero variance for $\hat Y$, we return NA with a clear warning. This occurs in regimes where the first-order delta method is not applicable (e.g., trimming, very weak identification). Bootstrap variance is recommended in those cases.

#### Variance Identities (Used in Code)

To avoid forming explicit inverses and to preserve non-negativity when $B$ is PSD, we use the following algebraically equivalent identities:

- Mean (scalar):

\[
\mathrm{Var}(\hat Y) \;=\; \nabla g^T \,(A^{-1} B A^{-T})\, \nabla g \;=\; x^T B x,\quad \text{where } A^T x = \nabla g.
\]

- Response-model coefficients (matrix): let $E_\beta$ select the $\beta$ coordinates in $\theta$ (same ordering as the stacked system). Then

\[
\mathrm{Var}(\hat\beta) \;=\; \big(A^{-T} E_\beta\big)^T B \big(A^{-T} E_\beta\big) \;=\; X_\beta^T B X_\beta,\quad \text{where } A^T X_\beta = E_\beta.
\]

Both identities equal the corresponding blocks of the sandwich $A^{-1} B A^{-T}$ in exact arithmetic, but they are numerically more stable. The implementation uses these two-solve forms for the mean and for $\mathrm{Var}(\hat\beta)$.

Remarks:

- The “minus sign” seen in some derivations for $-A^{-1}$ cancels inside the sandwich.
- With finite weight trimming, $g(\theta)$ is non-smooth; delta variance is not applicable. Prefer `variance_method = "bootstrap"`.

### Practical Identifiability and Diagnostics

The EL system balances the parametric response-model score against calibration constraints. Identifiability can weaken in the following situations:

- Weak or nearly collinear auxiliaries: if $X_i-\mu_x$ have little variation or are nearly collinear with the response score direction, the constraint block in $A=\partial F/\partial\theta$ becomes ill-conditioned.
- Inconsistent auxiliary means: if supplied $\mu_x$ are far from what the respondent sample can support (under the response model), denominators $D_i$ cluster near 0 and $\kappa(A)$ inflates.
- Heavy nonresponse or near-boundary $W$: when $W$ approaches 0 or 1, $\lambda_W=C/(1-W)$ can spike and amplify sensitivity.

Diagnostics exposed by the implementation help assess these issues:

- `jacobian_condition_number` ($\kappa(A)$), `max_equation_residual`, denominator summaries (min, lower quantiles, median), weight concentration (max share, top-5 share, ESS), and the trimming fraction.

Mitigations include standardizing predictors, trimming extreme weights (`trim_cap`), adding informative response-model predictors, and preferring bootstrap variance when diagnostics indicate fragility.

## Survey Design Details

We extend QLS's methodology to complex surveys in two complementary ways:

- **Estimating equations with base weights:** All sums already include the base weight $a_i$; set $a_i$ to the survey design weight for respondents. Totals $N_{\text{pop}}$ and $n_{\text{resp\_weighted}}=\sum a_i$ are computed from the design weights, which feeds into $\lambda_W = ((N_{\text{pop}}/n_{\text{resp\_weighted}}) - 1)/(1-W)$.

- **Design-based variance for totals:** Rather than assuming i.i.d. sampling, we estimate $B = \operatorname{Var}(\sum U_i)$ using the design:
  - Create full-design score variables by placing the respondent contributions $U_i$ on observed rows and zeros on nonrespondents.
  - Compute survey totals `svytotal(~U, design)` and take their covariance via `vcov(...)` to obtain $\widehat B$.

In short form, we compute

\[
\widehat B \;=\; \operatorname{vcov}\!\big(\,\texttt{svytotal}(\,\sim U,\; \texttt{design}\,)\,\big),
\]

where $U$ stacks the respondent score contributions and zeros elsewhere.

This matches the paper's guidance to adapt the likelihood/estimating framework to stratification or unequal-probability sampling. Our approach keeps the EL structure and uses standard survey inference tools for the second-order properties.

Degrees-of-freedom: For confidence intervals, we use survey degrees-of-freedom (t-quantiles) when a `survey.design` is supplied; otherwise, we use normal quantiles.

## Scaling and Unscaling

### Scaling (optional; `standardize=TRUE`)

- **Compute a `nmar_scaling_recipe`**: for each column $j$ in $Z$ and $X$ (excluding intercept):
  - $\text{mean}_j$, $\text{sd}_j$; if $\text{sd}_j \approx 0$, set $\text{sd}_j = 1$ to avoid blow-ups.
- **Transform**:
  - $Z_{\text{scaled}}[,j] = (Z_{\text{un}}[,j] - \text{mean}_j) / \text{sd}_j$
  - $X_{\text{scaled}}[,j] = (X_{\text{un}}[,j] - \text{mean}_j) / \text{sd}_j$
  - $\mu_{x,\text{scaled}}[j] = (\mu_{x,\text{un}}[j] - \text{mean}_j) / \text{sd}_j$

### Unscaling $\beta$ and vcov

- **Construct linear map** $D$ of size $K \times K$:
  - For columns $j \neq$ intercept: $D[j,j] = 1/\text{sd}_j$
  - For intercept: adjust to absorb centering: $D[\text{intercept},j] = -\text{mean}_j/\text{sd}_j$
- **Transform**: $\beta_{\text{unscaled}} = D \beta_{\text{scaled}}$; $\text{vcov}_{\text{unscaled}} = D \, \text{vcov}_{\text{scaled}} \, D^T$

Code: centralized in `src_dev/shared/scaling.R`; engines call `validate_and_apply_nmar_scaling()` and `unscale_coefficients()`.

## Bootstrap Variance

- **IID**:
  - Resample rows with replacement ($n$ to $n$), re-run estimator, compute $\text{var}$ of bootstrap $\hat{Y}$s; warn if many failures; return $\sqrt{\text{var}}$.
- **Survey**:
  - Convert to bootstrap replicate-weight design via `svrep::as_bootstrap_design`.
  - For each replicate, re-construct a temporary design and run estimator; use `survey::svrVar` to compute variance of replicate estimates (with scale/rscales).

Code mapping:

- Engine: `el_engine(..., family, standardize, trim_cap, variance_method, ...)` in `src_dev/engines/el/engine.R`
- Dispatch: `run_engine.nmar_engine_el(...)` in `src_dev/engines/el/run_engine.R` adapts the formula and forwards arguments to internal `el()` methods.
  - `el.data.frame()` / `el.survey.design()` in `src_dev/engines/el/impl/dataframe.R` and `src_dev/engines/el/impl/survey.R` prepare inputs, call `el_estimator_core()`, and wrap results.
- EL Core: `el_estimator_core(...)` in `src_dev/engines/el/impl/core.R` runs:
  - Construct $F(\theta)$ via `el_build_equation_system()` (`src_dev/engines/el/impl/equations.R`).
  - Solve $F(\theta)=0$ via `nleqslv` (Newton with analytic Jacobian when available, Broyden fallback).
  - Build EL weights, mean, and diagnostics.
- Jacobian: `build_el_jacobian(...)` in `src_dev/engines/el/impl/jacobian.R` returns analytic A whenever family supplies `d2mu.deta2` (logit, probit).
- Variance: `src_dev/engines/el/impl/variance.R` assembles B and computes delta variance; bootstrap variance in `src_dev/shared/bootstrap.R`.
- S3 result: `src_dev/engines/el/s3.R` provides print/tidy/glance and accessors.

### Practical Notes

- Denominator guard: $D_i \ge \varepsilon$ (default $10^{-8}$) across all steps; diagnostics report extreme fractions.
- Eta cap option: you can adjust the $\eta$ cap via `options(nmar.eta_cap = 60)` (default is 50) to suit your data scale and link

### Algorithm

We solve the full stacked system $F(\theta)=0$ with Newton using the analytic Jacobian $A = \partial F/\partial \theta$ and globalization via `nleqslv`. Denominator positivity ($\min_i D_i \ge \varepsilon$), predictor standardization, and capped $\eta$ ensure numerical stability. The estimating equations remain those of Qin, Leung and Shao (2002).

```text
Input: Z (response design), X (auxiliary design), mu_x (population means),
       a (base weights), family (logit/probit), trim_cap, tolerances.
Initialize: beta = 0 in scaled space (or user-supplied start),
            z = logit(observed response rate), lambda_x = 0.
Repeat until convergence of F(theta) = 0:
  1) Compute eta = Z beta, w = linkinv(eta), W = plogis(z),
     lambda_W = ((N_pop/n_resp_weighted) - 1)/(1 - W).
  2) Evaluate full stacked equations using guarded denominators
     D_i = 1 + lambda_W (w_i - W) + (X_i - mu_x)^T lambda_x.
  3) Compute analytic Jacobian A = dF/dtheta.
  4) Newton step: solve A * step = -F with globalization; enforce min D_i >= eps.
  5) Update theta <- theta + step.
Return: p_i \propto a_i / D_i and \hat{Y} = Sum p_i Y_i / Sum p_i.
```

## References

- Qin, J., Leung, D., and Shao, J. (2002). Estimation with survey data under nonignorable nonresponse or informative sampling. Journal of the American Statistical Association, 97(457), 193-200. doi:10.1198/016214502753479338

## Appendix: EL Engine API Reference (User-Facing)

This appendix summarizes the key options of the EL engine (constructor: `el_engine()`), their defaults, and recommended usage.

- **family** (default: "logit")
  - Values: "logit", "probit", or a family object (list with `name`, `linkinv`, `mu.eta`, `d2mu.deta2`, `score_eta`).
  - Notes: We implement `logit_family()` and `probit_family()`. Both use the log-likelihood score `score_eta(eta, delta) = mu.eta(eta)/linkinv(eta)` (for respondents), i.e., $\partial\log p/\partial\eta$. This matches the paper's semiparametric MLE equations and keeps the analytic Jacobian family-agnostic.

- **standardize** (default: TRUE)
  - Standardize $Z$/$X$ (and $\mu_x$) using a `nmar_scaling_recipe` for numerical stability. Coefficients and vcov are unscaled after solving.

- **trim_cap** (default: Inf)
  - Caps EL weights and redistributes mass. Improves robustness when extreme weights arise. Prefer `variance_method = "bootstrap"` when trimming is finite.

- **variance_method** (default: "delta")
  - "delta": analytic delta method variance via two linear solves for the sandwich. Returns NA with a clear warning when trimmed or when solves are numerically fragile.
  - "bootstrap": IID resampling or survey replicate weights via `svrep`; preferred with trimming or near-boundary cases.

- bootstrap_reps (default: 500)
  - Number of bootstrap replicates for `variance_method = "bootstrap"`. Increase for stability, decrease for speed.

- **control** (default: list())
  - Passed to `nleqslv` (e.g., `ftol`, `xtol`, `maxit`).

### Diagnostics (glance/print)

- `jacobian_condition_number` ($\kappa(A)$), `max_equation_residual`, denominator summaries (min, 1%/5%/median), weight concentration (max share/top-5/ESS), and trimming fraction. These help assess identification and numerical stability.

### Recommended settings

- Default: `variance_method = "delta"`, `standardize = TRUE`.
- With trimming or suspected weak identification: prefer `variance_method = "bootstrap"`.
- If A appears ill-conditioned or delta returns NA: prefer `variance_method = "bootstrap"` and report diagnostics (e.g., `jacobian_condition_number`). The package does not apply ridge or pseudoinverse at runtime.
