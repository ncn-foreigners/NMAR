---
title: "Developing a New NMAR Estimator"
description: >
  Guide for adding a new NMAR estimator to the `nmar` package
output: rmarkdown::html_document
vignette: >
  %\VignetteIndexEntry{Developing a New NMAR Estimator}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Developing a New NMAR Estimator in the `nmar` Package

This guide explains how to add and integrate a new Not‑Missing‑At‑Random (NMAR)
statistical estimator into `nmar`, following the architecture used by the
Empirical Likelihood (EL) engine, emphasizing a clean separation of concerns:

- Shared, reusable utilities live in `src_dev/shared/` (scaling, families,
  bootstrap, numerics, diagnostics).
- Estimator‑specific math and code live in `src_dev/engines/<method>/`.

> Repository conventions
> - Always edit source under `src_dev/`; the flat files in `R/` are generated by
>   running `Rscript build_r_folder.R`.
> - After changing `src_dev/`, regenerate roxygen outputs with
>   `R -q -e "devtools::document()"` and confirm `git status` is clean; this keeps
>   S3 registrations and Rd files in sync.
> - Use testthat v3 under `tests/testthat/` for tests and run
>   `R -q -e "devtools::test()"` before opening a PR.
> - Use roxygen2 for documentation; do not edit `NAMESPACE` directly.

***
## 0) General guidelines
1. Adhere to the functional programming style and design patterns (writing pure functions, function factories, functionals, and function operators) when appropriate (eg. building systems of equations)
2. Do not use unpredictable functions like sapply, use vapply or purrr equivalents instead
3. Avoid code duplication, reuse the shared infrastructure as much as possible
4. Develop loosely coupled, modular architecture, identify shareable, method-agnostic functionalities
5. Use S3 OOP system
6. Use constructor and validator functions for your S3 objects
7. Follow the best software engineering practices, including SOLID, DRY, KISS, YAGNI, Separation of Concerns, Principle of Least Astonishment, and Law of Demeter?

## 1) Overview of the package flow

User workflow:

1. User creates an engine configuration (e.g., `el_engine(...)`).
2. User calls `nmar(formula = ..., data = ..., engine = <engine>)`.
3. `nmar()` creates an `nmar_input_spec` via `parse_nmar_spec()`, validates it with
   `validate_nmar_args()` (using engine-specific traits), and then dispatches to
   `run_engine(engine, task)` by S3.
4. The engine’s `run_engine.<engine_class>()` extracts what it needs from `task`
    (typically via `prepare_nmar_design()`) and passes control to a
    method-specific generic, which dispatches on `data` type:
   - `*.data.frame(...)` for IID data
   - `*.survey.design(...)` for complex surveys
5. The method computes the estimator and returns a result object of class
   `c("nmar_result_<method>", "nmar_result")` containing the primary estimand
   and supporting data for S3 methods.

***

## 2) What to implement for a new estimator

Assume your engine will be named `<method>` (e.g., `exptilt2`, `abc`, etc.).

Create the following files under `src_dev/engines/<method>/`:

### 1) `engine.R` — engine constructor

```{r eval=FALSE}
method_engine <- function(...) {
# Validate controls using validate_arguments.R as in exptilt.
  validator$assert_choice(family, choices = c("logit", "probit"), name = "family")

  engine <- list(...)
  class(engine) <- c("nmar_engine_method", "nmar_engine")
  engine
}
```

Important: add `@keywords engine` in roxygen
### 2) `run_engine.R` — run_engine method

```{r eval=FALSE}
run_engine.nmar_engine_method <- function(engine, task) {
  design <- prepare_nmar_design(
    task,
    standardize = engine$standardize,
    auxiliary_means = engine$auxiliary_means,
    include_response = TRUE,
    include_auxiliary = TRUE
  )

  args <- c(
    list(
      data = design$survey_design %||% design$data,
      formula = task$formula,
      auxiliary_means = design$auxiliary_means,
      standardize = design$standardize
    ),
    as.list(engine)
  )

  method_fn <- get("method", mode = "function")
  fit <- do.call(method_fn, args)
  if (!inherits(fit, "nmar_result_method")) stop("Expected nmar_result_method.")
  fit
}
```

The engine should rely on the shared validation/traits infrastructure:

```r
engine_traits.nmar_engine_method <- function(engine) {
  utils::modifyList(
    engine_traits.default(engine),
    list(
      allow_outcome_in_missingness = TRUE,      # customise as needed
      allow_covariate_overlap = FALSE,
      requires_single_outcome = TRUE
    )
  )
}
```

### 3) `impl/<method>.R` — define the generic

```{r eval=FALSE}
#' @param data A data.frame or survey.design.
#' @export
method <- function(data, ...) UseMethod("method")
```

### 4) `impl/<method>_dataframe.R` — method for IID data

```{r eval=FALSE}
#' @export
method.data.frame <- function(data, formula, controls = list(), ...) {
# Prepare inputs. You may adapt from EL’s el_prepare_inputs() pattern.
  parsed <- build_method_inputs(data, formula, controls)
  estimation_data <- parsed$data
  internal_formula <- parsed$formula_list

# Identify respondents
  response_var <- all.vars(internal_formula$response)[1]
  obs_idx <- which(estimation_data[[response_var]] == 1)

# Build design matrices
  Z_un <- model.matrix(update(internal_formula$response, NULL ~ .), data = estimation_data[obs_idx, ])
  X_un <- build_method_auxiliary_matrix(estimation_data[obs_idx, ])
  mu_x <- controls$auxiliary_means

# Shared scaling
  sc <- validate_and_apply_nmar_scaling(
    standardize = TRUE, has_aux = !is.null(X_un),
    response_model_matrix_unscaled = Z_un,
    auxiliary_matrix_unscaled = if (is.null(X_un)) matrix(nrow = nrow(Z_un), ncol = 0) else X_un,
    mu_x_unscaled = mu_x
  )

# Solve on the scaled space (method-specific code)
  fit <- method_solve_core(
    full_data = estimation_data,
    respondent_data = estimation_data[obs_idx, ],
    response_model_matrix_scaled = sc$response_model_matrix_scaled,
    auxiliary_matrix_scaled = sc$auxiliary_matrix_scaled,
    mu_x_scaled = sc$mu_x_scaled,
    controls = controls,
    ...
  )

# Wrap into a standard result object (see Section 5)
  new_nmar_result_method(
    estimate = fit$estimate,
    std_error = fit$std_error,
    coefficients = fit$model$coefficients,
    vcov = fit$model$vcov,
    diagnostics = fit$diagnostics
  )
}
```

### 5) `impl/<method>_survey.R` — method for survey.design

Follow EL’s `el.survey.design` pattern:

- Subset respondents via `observed_mask`.
- Get respondent base weights via `weights(resp_design)` and set `N_pop <- sum(weights(design))`.
- For survey designs that require design-based variance, you may compute the
  covariance of score totals via `svytotal(~ scores, design)` followed by
  `vcov()`. For EL, analytical delta variance is disabled; use bootstrap
  variance instead.

### 6) `impl/<method>_equations.R` / `impl/<method>_jacobian.R` (optional)

If your estimator uses a system of estimating equations and an analytic
Jacobian (like EL), define and document them here. Otherwise, keep method‑specific
math here (e.g., likelihood, gradients, constraints).

### 7) `impl/<method>_variance.R`

- If applicable for your method, assemble $A = \\partial F/\\partial \\theta$, $B = Var(\\sum scores)$ and the gradient
  of the functional $g(\\theta)$ for an analytical delta variance. For EL, analytical
  delta variance is currently disabled and the engine returns NA when requested.
- Add an option to use shared bootstrap variance helpers (`bootstrap_variance()`) for IID or survey designs.

### 8) `impl/<method>_constructors.R`
Call `new_nmar_result()` inside your constructor so the object carries class
`c("nmar_result_<method>", "nmar_result")` and shares the standard layout:

- Scalar fields: `estimate`, `estimate_name`, `se`, `converged`.
- Lists: `model` (coefficients/vcov), `weights_info`, `sample`, `inference`,
  `diagnostics`, `meta`, `extra`.

Populate these components directly; do not add extra top-level aliases such as
`y_hat` or `data_info`. Call `validate_nmar_result()` before returning to ensure
the structure is complete.

Minimal constructor example (inside `src_dev/engines/<method>/impl/constructors.R`):

```r
#' @keywords internal
new_nmar_result_method <- function(estimate, se,
                                   coefficients = NULL, vcov = NULL,
                                   weights = NULL, sample = list(),
                                   inference = list(variance_method = NA_character_,
                                                    df = NA_real_,
                                                    message = NA_character_),
                                   diagnostics = list(),
                                   meta = list(engine_name = "method", call = NULL, formula = NULL),
                                   extra = list()) {
  sample_defaults <- list(n_total = NA_integer_, n_respondents = NA_integer_,
                          is_survey = FALSE, design = NULL, outcome_var = NA_character_)
  sample <- utils::modifyList(sample_defaults, sample)
  result <- new_nmar_result(
    estimate = estimate,
    estimate_name = sample$outcome_var,
    se = se,
    converged = TRUE,
    model = list(coefficients = coefficients, vcov = vcov),
    weights_info = list(values = weights, trimmed_fraction = NA_real_),
    sample = list(n_total = sample$n_total,
                  n_respondents = sample$n_respondents,
                  is_survey = sample$is_survey,
                  design = sample$design),
    inference = inference,
    diagnostics = diagnostics,
    meta = meta,
    extra = extra,
    class = "nmar_result_method"
  )
  validate_nmar_result(result, "nmar_result_method")
}
```

### 9) `s3.R` — S3 methods for your result class

Implement (or rely on parent defaults):

- `print.nmar_result_<method>()`, `summary.nmar_result_<method>()` (optional).
  When overriding, call `NextMethod()` first to reuse the parent output and then
  append estimator-specific details.
- `estimate()`, `vcov()`, `confint()`, `tidy()`, `glance()`: these work out of
  the box as long as you populate the shared schema.
- Optional accessors: `weights()`, `fitted()`, `coef()`, etc., if your engine
  exposes them.

***

## 3) Reusing shared infrastructure

Use these shared modules in `src_dev/shared/`:

### Scaling

- `validate_and_apply_nmar_scaling()` to standardize `Z`/`X` and auxiliary means
  before solving, returning an `nmar_scaling_recipe` for unscaling.
- `unscale_coefficients()` to map scaled coefficients and vcov back to the
  original scale for reporting.
- Intercept is never scaled; constant columns get sd=1. Engines may create a
  recipe from respondents‑only matrices (as in EL) or from full/design matrices.

### Families (response model)

- `logit_family()` / `probit_family()` expose `linkinv`, `mu.eta`, `d2mu.deta2`,
  and `score_eta` for link‑agnostic estimating equations and Jacobians.
- Probit uses a tail‑stable log‑ratio for `phi/Phi`.

### Bootstrap variance

- `bootstrap_variance()` S3 generic with methods for `data.frame` and
  `survey.design`.
- IID: resample rows and rerun the estimator.
- Survey: convert to bootstrap replicate weights (`svrep::as_bootstrap_design`),
  use `survey::withReplicates`, then `survey::svrVar`.

### 4) Engine boundaries and naming

- Do not call helpers from other engines’ `impl/` folders. Engines must depend only on `src_dev/shared/` and their own `src_dev/engines/<method>/impl/`.
- Prefix engine‑specific helpers with `<method>_` (e.g., `el_...` for empirical likelihood) to signal ownership and avoid accidental reuse.
- The parent S3 class `nmar_result` provides default methods (`estimate`, `vcov`, `confint`, `tidy`, `glance`, `plot`/`autoplot`, `coef`, `fitted`, `weights`, `formula`). Implement child print/summary only if you need extra presentation.
