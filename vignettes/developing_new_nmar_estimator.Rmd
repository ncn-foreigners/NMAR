---
title: "Developing a New NMAR Estimator"
description: >
  Guide for adding a new NMAR estimator to the `nmar` package
output: rmarkdown::html_document
vignette: >
  %\VignetteIndexEntry{Developing a New NMAR Estimator}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Developing a New NMAR Estimator in the `nmar` Package

This guide explains how to add and integrate a new Not‑Missing‑At‑Random (NMAR)
statistical estimator into `nmar`, following the package’s plugin architecture
used by the Empirical Likelihood (EL) and Exponential Tilting (ET) engines. It
emphasizes a clean separation of concerns and reuse of shared infrastructure:

- Shared, reusable utilities live in `src_dev/shared/` (scaling, families,
  bootstrap, numerics, diagnostics).
- Estimator‑specific math and code live in `src_dev/engines/<method>/`.

> Repository conventions
> - Always edit source under `src_dev/`; the flat files in `R/` are generated by
>   running `Rscript build_r_folder.R`.
> - After changing `src_dev/`, regenerate roxygen outputs with
>   `R -q -e "devtools::document()"` and confirm `git status` is clean; this keeps
>   S3 registrations and Rd files in sync.
> - Use testthat v3 under `tests/testthat/` for tests and run
>   `R -q -e "devtools::test()"` before opening a PR.
> - Use roxygen2 for documentation; do not edit `NAMESPACE` directly.

***
## 0) General guidelines
1. Adhere to the functional programming style and design patterns (write pure functions; use function factories/functionals/operators where appropriate, e.g., for systems of equations)
2. Do not use unpredictable functions like sapply, use vapply or purrr equivalents instead
3. Avoid code duplication, reuse the shared infrastructure as much as possible
4. Develop loosely coupled, modular architecture, identify shareable, method-agnostic functionalities
5. Use S3 OOP system
6. Use constructor and validator functions for your S3 objects
7. Follow the best software engineering practices, including SOLID, DRY, KISS, YAGNI, Separation of Concerns, Principle of Least Astonishment, and Law of Demeter.

## 1) Overview of the package flow

User workflow:

1. User creates an engine configuration (e.g., `el_engine(...)`).
2. User calls `nmar(formula = ..., data = ..., engine = <engine>)`.
3. `nmar()` creates an `nmar_input_spec` via `parse_nmar_spec()`, validates it with
   `validate_nmar_args()` (using engine-specific traits), and then dispatches to
   `run_engine(engine, task)` by S3.
4. The engine’s `run_engine.<engine_class>()` extracts what it needs from `task`
    (typically via `prepare_nmar_design()`) and passes control to a
    method-specific generic, which dispatches on `data` type:
   - `*.data.frame(...)` for IID data
   - `*.survey.design(...)` for complex surveys
5. The method computes the estimator and returns a result object of class
   `c("nmar_result_<method>", "nmar_result")` containing the primary estimand
   and supporting data for S3 methods.

***

## 2) What to implement for a new estimator

Assume your engine will be named `<method>` (e.g., `exptilt2`, `abc`, etc.).

Create the following files under `src_dev/engines/<method>/`:

### 1) `engine.R` — engine constructor

```{r eval=FALSE}
#' My Method engine for NMAR
#'
#' @keywords engine
method_engine <- function(
    standardize = TRUE,
    variance_method = c("delta", "bootstrap", "none"),
    auxiliary_means = NULL,
    control = list(),
    ...) {
  variance_method <- match.arg(variance_method)

  validator$assert_logical(standardize, name = "standardize")
  validator$assert_choice(variance_method, choices = c("delta", "bootstrap", "none"), name = "variance_method")
  validator$assert_named_numeric(auxiliary_means, name = "auxiliary_means", allow_null = TRUE)
  validator$assert_list(control, name = "control")

  engine <- list(
    standardize     = standardize,
    variance_method = variance_method,
    auxiliary_means = auxiliary_means,
    control         = control,
    ...
  )
  class(engine) <- c("nmar_engine_method", "nmar_engine")
  engine
}

#' Canonical engine name
#' @export
engine_name.nmar_engine_method <- function(x) "my_method"

#' Engine traits (validation policy)
#' @export
engine_traits.nmar_engine_method <- function(engine) {
  utils::modifyList(
    engine_traits.default(engine),
    list(
      allow_outcome_in_missingness = FALSE, # customize as needed
      allow_covariate_overlap      = FALSE, # customize as needed
      requires_single_outcome      = TRUE,
# If your method supports respondents-only inputs when N is known,
# set allow_respondents_only accordingly.
      allow_respondents_only       = FALSE
    )
  )
}
```
### 2) `run_engine.R` — run_engine method

```{r eval=FALSE}
#' @exportS3Method run_engine nmar_engine_method
run_engine.nmar_engine_method <- function(engine, task) {
# Prepare standardized design and metadata shared across engines
  design <- prepare_nmar_design(
    task,
    standardize       = engine$standardize,
    auxiliary_means   = engine$auxiliary_means,
    include_response  = TRUE,
    include_auxiliary = TRUE
  )

# Rebuild a partition-aware formula y ~ aux | response for clarity/logging
  f_use <- nmar_rebuild_partitioned_formula(
    base_formula      = design$engine_formula,
    response_rhs_lang = design$response_rhs_lang,
    aux_rhs_lang      = design$aux_rhs_lang,
    env               = task$environment
  )

  args <- list(
    data            = design$survey_design %||% design$data,
    formula         = f_use,
    user_formula    = design$user_formula,
    design_matrices = design$design_matrices,
    auxiliary_means = design$auxiliary_means,
    standardize     = design$standardize,
    outcome_label   = design$outcome_label,
    control         = engine$control,
    variance_method = engine$variance_method,
    trace_level     = task$trace_level
  )
  if (!isTRUE(design$is_survey)) {
    args$design_weights <- design$weights
  }

  fit <- do.call(method, args)
  if (!inherits(fit, "nmar_result_method")) stop("Expected class 'nmar_result_method'.")
  fit
}
```

### 3) `impl/<method>.R` — define the generic

```{r eval=FALSE}
#' @param data A data.frame or survey.design.
#' @export
method <- function(data, ...) UseMethod("method")
```

### 4) `impl/<method>_dataframe.R` — method for IID data

```{r eval=FALSE}
#' @export
method.data.frame <- function(data, formula,
                              user_formula = formula,
                              design_matrices,
                              auxiliary_means = NULL,
                              standardize = TRUE,
                              design_weights = NULL,
                              outcome_label = NULL,
                              control = list(),
                              variance_method = c("delta", "bootstrap", "none"),
                              trace_level = 0,
                              ...) {
  variance_method <- match.arg(variance_method %||% "none")
  stopifnot(!is.null(design_matrices))

# Optional: If your method needs respondent-only matrices, subset
# design_matrices$response / $auxiliary to observed indices here.

# fit <- method_solve_core(..., design_matrices = design_matrices, ...)

  new_nmar_result_method(
    estimate = fit$estimate, # becomes $y_hat
    se = fit$se,
    coefficients = fit$model$coefficients,
    vcov = fit$model$vcov,
    diagnostics = fit$diagnostics,
    sample = list(
      n_total       = nrow(data),
      n_respondents = sum(!is.na(model.response(model.frame(formula, data, na.action = na.pass)))),
      is_survey     = FALSE,
      design        = NULL
    ),
    inference = list(variance_method = variance_method, df = NA_real_, message = NA_character_),
    meta = list(engine_name = "my_method", call = match.call(), formula = user_formula)
  )
}
```

### 5) `impl/<method>_survey.R` — method for survey.design

Follow EL/ET survey patterns:

- Subset respondents via an observed mask derived from your response indicator.
- Extract respondent base weights via `stats::weights(resp_design)` and set
  `N_pop <- sum(stats::weights(design))` as the population total (unless your
  method uses a different scale).
- Prefer the shared bootstrap helpers for variance in complex designs.

### 6) `impl/<method>_equations.R` / `impl/<method>_jacobian.R` (optional)

If your estimator uses a system of estimating equations and an analytic
Jacobian (like EL), define and document them here. Otherwise, keep method‑specific
math here (e.g., likelihood, gradients, constraints).

### 7) `impl/<method>_variance.R`

- If applicable, assemble an analytical delta variance using your method’s
  estimating system and gradients. For EL, analytical delta variance is disabled.
- Prefer the shared bootstrap variance helpers (`bootstrap_variance()`) for IID
  and survey designs (see `src_dev/shared/bootstrap.R`).

### 8) `impl/<method>_constructors.R`
Call `new_nmar_result()` inside your constructor so the object carries class
`c("nmar_result_<method>", "nmar_result")` and shares the standard layout:

- Scalar fields: `estimate` (stored as `$y_hat`), `estimate_name`, `se`, `converged`.
- Lists: `model` (coefficients/vcov), `weights_info`, `sample`, `inference`,
  `diagnostics`, `meta`, `extra`.

Populate these components directly and call `validate_nmar_result()` before
returning to ensure the structure is complete.

Minimal constructor example (inside `src_dev/engines/<method>/impl/constructors.R`):

```r
#' @keywords internal
new_nmar_result_method <- function(estimate, se,
                                   coefficients = NULL, vcov = NULL,
                                   weights = NULL, sample = list(),
                                   inference = list(variance_method = NA_character_,
                                                    df = NA_real_,
                                                    message = NA_character_),
                                   diagnostics = list(),
                                   meta = list(engine_name = "method", call = NULL, formula = NULL),
                                   extra = list()) {
  sample_defaults <- list(n_total = NA_integer_, n_respondents = NA_integer_,
                          is_survey = FALSE, design = NULL, outcome_var = NA_character_)
  sample <- utils::modifyList(sample_defaults, sample)
  result <- new_nmar_result(
    estimate = estimate,
    estimate_name = sample$outcome_var,
    se = se,
    converged = TRUE,
    model = list(coefficients = coefficients, vcov = vcov),
    weights_info = list(values = weights, trimmed_fraction = NA_real_),
    sample = list(n_total = sample$n_total,
                  n_respondents = sample$n_respondents,
                  is_survey = sample$is_survey,
                  design = sample$design),
    inference = inference,
    diagnostics = diagnostics,
    meta = meta,
    extra = extra,
    class = "nmar_result_method"
  )
  validate_nmar_result(result, "nmar_result_method")
}
```

### 9) `s3.R` — S3 methods for your result class

Implement (or rely on parent defaults):

- `print.nmar_result_<method>()`, `summary.nmar_result_<method>()` (optional).
  When overriding, call `NextMethod()` first to reuse the parent output and then
  append estimator-specific details.
- Parent methods provided: `vcov()`, `confint()`, `tidy()`, `glance()`,
  `coef()`, `fitted()`, `weights()`, `formula()`, and `se()`.
- Optional accessors: implement only if your engine needs extra presentation.

S3 registration (roxygen):

```r
#' @exportS3Method run_engine nmar_engine_method
#' @exportS3Method print nmar_result_method
#' @exportS3Method summary nmar_result_method
```

Run `devtools::document()` to update `NAMESPACE`.

***

## 3) Reusing shared infrastructure

Use these shared modules in `src_dev/shared/`:

### Scaling

- `validate_and_apply_nmar_scaling()` to standardize `Z`/`X` and auxiliary means
  before solving, returning an `nmar_scaling_recipe` for unscaling.
- `unscale_coefficients()` to map scaled coefficients and vcov back to the
  original scale for reporting.
- Intercept is never scaled; constant columns get sd=1. Engines may create a
  recipe from respondents‑only matrices (as in EL) or from full/design matrices.

### Families (response model)

- `logit_family()` / `probit_family()` expose `linkinv`, `mu.eta`, `d2mu.deta2`,
  and `score_eta` for link‑agnostic estimating equations and Jacobians.
- Probit uses a tail‑stable log‑ratio for `phi/Phi`.

### Bootstrap variance

- `bootstrap_variance()` S3 generic with methods for `data.frame` and
  `survey.design`.
- IID: resample rows and rerun the estimator with `future.apply` (optionally
  instrumented via `progressr`).
- Survey: convert to bootstrap replicate weights via
  `svrep::as_bootstrap_design()` and compute variance using `survey::svrVar()`.

### 4) Engine boundaries and naming

- Do not call helpers from other engines’ `impl/` folders. Engines must depend only on `src_dev/shared/` and their own `src_dev/engines/<method>/impl/`.
- Prefix engine‑specific helpers with `<method>_` (e.g., `el_...` for empirical likelihood) to signal ownership and avoid accidental reuse.
- The parent S3 class `nmar_result` provides default methods (`vcov`, `confint`,
  `tidy`, `glance`, `coef`, `fitted`, `weights`, `formula`, `se`). Implement child
  print/summary only if you need extra presentation.

***

## 5) Testing your new engine (checklist)

- Convergence on a simple synthetic IID case; verify `converged` is TRUE and
  `y_hat` matches a manual check (e.g., via `weights(res)`).
- Factor/transforms in the formula (e.g., `I(X^2)`, factors) behave as expected;
  snapshot `print()` and `summary()` output.
- Survey path (`survey.design`): ensure `weights(res, scale = "population")`
  sums to `N_pop` and confidence intervals use t-quantiles when degrees of
  freedom are finite.
- Failure modes: set `on_failure = "return"` in your engine and assert that
  non-convergence populates `diagnostics$message` and returns a valid result
  object with `converged = FALSE`.
- If using an analytic Jacobian, compare against `numDeriv::jacobian()` on small
  cases.

Run before committing:

```r
Rscript build_r_folder.R
devtools::document(); devtools::test(); devtools::check()
```
